zillow_scraper
==============
Final project for the online computer science class (Harvard's CS50x) in April 2014.

The general infrastructure (page rendering and MySQL integration) is not mine, it was adopted from one of the CS50x projects. The core scraping, parsing and MySQL processing functionality (my bread and butter) is in `client_1/brokers/`. Workflow for weekly cron job is `update_db`, then `update_retention`; `download` is user-driven whenever he wants to download updated results. 

The live app is at http://www.non-descript.net/, password "hbs". Scraping functionality no longer works (after Zillow re-designed its website in September 2014) and there was no business need to continue scraping. Weekly data collected between May and September 2014 is still stored and is available for download.

Code for the second part of the app (Intuitive Surgical scraping) is not posted on GitHub because it's just a much simpler version of Zillow's - single source page, no MySQL processing and no retention stats.

**Background information - implementation aspect**

Zillow's website at time (April 2014) included a broker directory section, organized in scopes at four levels: national (one scope), states (56), cities (~540), neighborhoods (~3,700). Each scope had up to 25 pages with up to 10 broker profiles on each. Broker's position within a scope was based on ratings, and one broker could be listed in several scopes (e.g. on once at the state level, once at the city level and then 4 times in different neighborhoods in that city).

The script (`update_db.php`) was set up as a cron job (LAMP stack) to scan the entire website every week, parsing broker profiles and separating those with free subscription from those with paid one (based on profile tokens). For example, one update in July traversed 17,448 webpages and found 61,290 brokers. The script was deliberately slowed down to take several seconds between each http request, so as to not alert the target, the above-mention July scan took almost 28 hours to complete.

After initial parsing, broker names and location were stored in a MyQSL database.



**Background information - business aspect**

Zillow is a publicly traded company, and about 80% of its revenue (at least at the time of project) was generated by selliing subscriptions to real estate brokers. Brokers with paid subscriptions had their listings displayed more prominently on the websidte and had access to additional marketing features, compared to their free subscription. Having a real-time sense of the subscriber count (all subscriptions, and paid ones in particular) is a valuable information for a Zillow investor.


